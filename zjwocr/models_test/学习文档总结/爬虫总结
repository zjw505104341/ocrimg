
一. 需求
    有爬虫需求文档的情况下:
        1. 先看文档（总结问题）
        2. 提问（最好带着答案去问）
    无文档的情况下:
        1. 问要这个数据干什么（行业经验，爬虫的角度分析，去采集那些数据源，爬虫圈内是否有同样的需求）
        2. 数据结构（都是需要那些字段）
        3. 数据及时性要求

二. 调研
    查看数据量级:
        开发是否有价值

    查看反扒:
        1. 查看难度
        2. 去网站寻找类似的答案

        web:
            （1. cookie, 2. 验证码, 3.字体反扒, 4. ip, 5.假数据, 6.headers, 7.登录, 8.弹幕, 9.js）

            js: 关键字先干一遍（AES,CBC,Base64,MD5）
            ip: 需要大量的测试
            headers:
              1. 请求headers顺序
              2. ua,host,rf,等
              技巧: ua  User-Agent:baiduspider
            cookie:
              1.服务段返回set-cookie,拿到set-cookie 在去请求（连环请求）
              2.服务段返回(token,key),客户端拿到这个值去（js）加密,去请求
              3.客户端生成发送服务端
            验证码:
              1. 常见的数字，图片验证码
              2. 人机验证类型（geetest,腾讯防水墙,网易,数美,螺丝帽等）
              3. 文字计算型（对用户不友好）
              4. gif图片（取当中的一帧进行训练）
            字体反爬:
              1. css文件
              2. 字体文件加密（猫眼价格，大众点评，土地网，美团web段）
              3. 返回的文字是图片
            假数据: 需要大量的测试,需要问同行（无道德，没有底线）
            登录: 要小心操作
            弹幕: 分析协议
        app:
            pass
        小程序:
            pass
        公众号:
            pass
    采集策略实现
       一. 优先去采集一部分数据
       二. 优化
           1.大量查看详情页，是否为id 自增类数据
           2.若不是自增的,想其他采集策略
       三. 全量,增量（一定要搞清楚）
    调度方案
        根据业务需求而制定（分钟，小时，天，礼拜）

二. 实现
    1. 选用技术框架，不同类型的爬虫，选取不同类型的爬虫框架
    2. 代码实现
    3. 数据入库
    4. 数据校验（人工校验，这一步每天都要看，可以精确定位到假数据）
    5. 数据结构（1.每天采集的时间  2.爬虫爬取时间  3.数据来源url  4.渠道id  5.组合值唯一数据id（组合生成））
    6. 爬虫架构
三. 交付

反扒:
    服务端
         (ip,cookie校验,token,假数据,请求headers顺序)
    客户端
         js解密返回报文
    服务端+客户端
        1. 多个请求
        2.